"""
SpaceAgent: A Spatially-Aware LLM Framework for Gene Set Analysis.
Target: ACL / NeurIPS / ISMB / Bioinformatics
Author: SpaceAgent Team
License: MIT
"""

import os
import json
import logging
import requests
import warnings
import gc
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple, Set, Union, Any

import numpy as np
import pandas as pd
import scanpy as sc
import squidpy as sq
import scipy.spatial
import torch
from scipy import sparse
from sklearn.neighbors import NearestNeighbors
from transformers import pipeline
from scipy.stats import ttest_ind
import random
import seaborn as sns

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as mcolors
import matplotlib.patheffects as path_effects

# ==========================================
# 1. é…ç½®ç®¡ç†
# ==========================================
@dataclass
@dataclass
# ==========================================
# 1. é…ç½®ç®¡ç† (Revised)
# ==========================================
@dataclass
class SpaceConfig:
    data_dir: str = "./gold_data"
    result_dir: str = "./results_gold"
    
    # çŸ¥è¯†åº“è·¯å¾„
    db_path: str = "omnipath_intercell.csv"
    kegg_path: str = "KEGG_2021_Human.gmt"
    go_bp_path: str = "GO_Biological_Process_2023.gmt"
    reactome_path: str = "Reactome_2022.gmt"
    
    # Data Processing Parameters
    min_counts: int = 50
    min_genes: int = 200
    n_marker_genes: int = 1000
    blacklist_prefixes: Tuple[str] = ("MT-", "RPS", "RPL", "HB", "MALAT1")
    spatial_dist_threshold: float = 250.0
    
    # --- LLM Settings (Modified) ---
    # mode options: 'local' (for Llama) or 'api' (for GPT-4)
    llm_source: str = "api" 
    
    # Local Model Settings
    model_id: str = None
    gpu_id: int = 0
    
    # OpenAI / GPT-4 Settings
    openai_api_key: str = "sk-XXX"  # <--- REPLACE WITH YOUR KEY
    openai_model: str = "gpt-4"     # Or 
    openai_base_url: str = "https://api.openai.com/v1" # Optional: For proxies
    
    max_new_tokens: int = 512

    def __post_init__(self):
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.result_dir, exist_ok=True)

# ==========================================
# 2. æ•°æ®ç®¡ç†
# ==========================================
class DataManager:
    def __init__(self, config: SpaceConfig):
        self.cfg = config
        self.lr_db = {}
        self.all_lr_genes = set()
        
        # [æ–°å¢] åŠŸèƒ½æ³¨é‡Šç¼“å­˜ (Gene -> List of Functions)
        self.func_db = {} 

    def _download_gmt(self, url, save_path):
        """è¾…åŠ©å‡½æ•°ï¼šä¸‹è½½ GMT æ ¼å¼æ•°æ®åº“"""
        if not os.path.exists(save_path):
            print(f"â¬‡ï¸ Downloading DB to {save_path}...")
            try:
                r = requests.get(url, timeout=30)
                if r.status_code == 200:
                    with open(save_path, 'w', encoding='utf-8') as f:
                        f.write(r.text)
            except Exception as e:
                logger.warning(f"âš ï¸ Download failed: {e}")

    def prepare_knowledge_base(self):
        # 1. OmniPath (ä¿æŒä¸å˜)
        if not os.path.exists(self.cfg.db_path):
            print("â¬‡ï¸ Downloading OmniPath...")
            try:
                import omnipath as op
                db = op.interactions.import_intercell_network(
                    transmitter_params={"categories": "ligand"},
                    receiver_params={"categories": "receptor"}
                )
                db.to_csv(self.cfg.db_path, index=False)
            except Exception: pass
        
        # åŠ è½½ OmniPath
        try:
            df = pd.read_csv(self.cfg.db_path, low_memory=False)
            # ... (ä¿ç•™åŸæœ‰çš„ OmniPath è§£æä»£ç ) ...
            cols = df.columns
            src_col = next((c for c in ['source_genesymbol', 'genesymbol_intercell_source'] if c in cols), None)
            tgt_col = next((c for c in ['target_genesymbol', 'genesymbol_intercell_target'] if c in cols), None)
            if src_col and tgt_col:
                for _, row in df.iterrows():
                    s, t = str(row[src_col]).upper().strip(), str(row[tgt_col]).upper().strip()
                    if s and t and s!=t:
                        self.all_lr_genes.add(s); self.all_lr_genes.add(t)
                        self.lr_db[(s, t)] = "LR"
        except: pass

        # 2. [æ–°å¢] ä¸‹è½½å¹¶åŠ è½½åŠŸèƒ½æ•°æ®åº“ (GO & Reactome)
        # ä½¿ç”¨ Enrichr çš„åº“æº
        self._download_gmt(
            "https://maayanlab.cloud/Enrichr/geneSetLibrary?mode=text&libraryName=GO_Biological_Process_2023",
            self.cfg.go_bp_path
        )
        self._download_gmt(
            "https://maayanlab.cloud/Enrichr/geneSetLibrary?mode=text&libraryName=Reactome_2022",
            self.cfg.reactome_path
        )

        # 3. [æ–°å¢] è§£æåŠŸèƒ½åº“åˆ°å†…å­˜ (æ„å»º Gene -> Function æ˜ å°„)
        self._load_func_db(self.cfg.go_bp_path, source="GO")
        self._load_func_db(self.cfg.reactome_path, source="Reactome")
        
        print(f"âœ… Knowledge Base Ready: OmniPath + {len(self.func_db)} functional annotations.")

    def _load_func_db(self, path, source="DB"):
        """è¯»å– GMT æ–‡ä»¶å¹¶å»ºç«‹åå‘ç´¢å¼•: Gene -> [Function1, Function2]"""
        if not os.path.exists(path): return
        try:
            with open(path, 'r', encoding='utf-8') as f:
                for line in f:
                    parts = line.strip().split('\t')
                    if len(parts) < 3: continue
                    term_name = parts[0]  # åŠŸèƒ½åç§°
                    genes = parts[2:]     # åŸºå› åˆ—è¡¨
                    
                    for g in genes:
                        g_upper = g.upper()
                        if g_upper not in self.func_db:
                            self.func_db[g_upper] = []
                        # é™åˆ¶æ¯ä¸ªåŸºå› æœ€å¤šå­˜ 5 æ¡æœ€å…·ä½“çš„åŠŸèƒ½ï¼Œé˜²æ­¢ Prompt çˆ†ç‚¸
                        if len(self.func_db[g_upper]) < 5:
                            # ç®€å•æ¸…æ´—ï¼šå»æ‰å¤ªé•¿çš„æè¿°
                            if len(term_name) < 80:
                                self.func_db[g_upper].append(f"{term_name} ({source})")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load {path}: {e}")

    def load_dataset(self, source: str = "squidpy_mouse_brain") -> str:
        """
        åŠ è½½æ•°æ®é›†ï¼Œæ”¯æŒè‡ªå®šä¹‰æ•°æ®
        """
        dataset_map = {
            # 1. åŸºå‡†: Mouse Brain (Visium)
            "squidpy_mouse_brain": {
                "filename": "visium_mouse_brain_hne.h5ad",
                "desc": "Visium H&E Mouse Brain",
                "type": "squidpy",
                "func": sq.datasets.visium_hne_adata
            },
            # 2. é«˜ç²¾: Mouse Embryo (seqFISH)
            "squidpy_seqfish": {
                "filename": "seqfish_mouse_embryo.h5ad",
                "desc": "seqFISH Mouse Embryo",
                "type": "squidpy",
                "func": sq.datasets.seqfish
            },
            # 4. ã€æ–°å¢ã€‘ç”¨æˆ·è‡ªå®šä¹‰: Human Embryo 3D
            "custom_human_embryo": {
                "filename": "human_embryo_3D.h5ad", # ç›®æ ‡æ–‡ä»¶å
                "desc": "Custom Human Embryo (3D Aligned)",
                "type": "local_file",
                "path": "./gold_data/human_embryo_3D.h5ad" # ä½ çš„åŸå§‹æ–‡ä»¶è·¯å¾„
            }
        }

        if source not in dataset_map:
            logger.error(f"Unknown data source: {source}")
            return None

        meta = dataset_map[source]
        
        # ç›®æ ‡ä¿å­˜è·¯å¾„ (ç»Ÿä¸€ç®¡ç†åœ¨ result_dir æˆ– data_dir)
        # æ³¨æ„ï¼šå¯¹äº local_fileï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨åŸå§‹è·¯å¾„ï¼Œæˆ–è€…ç”±ç”¨æˆ·æŒ‡å®š
        if meta["type"] == "local_file":
            save_path = meta["path"]
        else:
            save_path = os.path.join(self.cfg.data_dir, meta["filename"])
        
        # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(save_path):
            logger.info(f"âœ… Found existing data ({meta['desc']}): {save_path}")
            # å¦‚æœæ˜¯è‡ªå®šä¹‰æ–‡ä»¶ï¼Œæˆ‘ä»¬åšä¸€æ¬¡ç®€å•çš„è¯»å–æ£€æŸ¥ï¼Œç¡®ä¿å®ƒæ˜¯æœ‰æ•ˆçš„ h5ad
            if meta["type"] == "local_file":
                return save_path 
            # å¯¹äºä¸‹è½½ç±»æ•°æ®ï¼Œç»§ç»­åç»­é€»è¾‘...
            return save_path
            
        # 2. ä¸‹è½½ (é’ˆå¯¹éæœ¬åœ°æ–‡ä»¶)
        logger.info(f"â¬‡ï¸ Downloading {meta['desc']}...")
        try:
            if meta["type"] == "squidpy":
                adata = meta["func"]()
            elif meta["type"] == "squidpy_custom":
                adata = sq.datasets.visium(sample_id=meta["sample_id"], include_hires_tiff=True)
            elif meta["type"] == "local_file":
                logger.error(f"âŒ Custom file not found at: {save_path}")
                return None

            # 3. ç»Ÿä¸€é¢„å¤„ç† (åŒå‰)
            adata.var_names = [str(g).upper() for g in adata.var_names]
            adata.var_names_make_unique()
            
            # å…¼å®¹å¤„ç†... (çœç•¥ï¼Œä¸ä¹‹å‰ä¸€è‡´)
            if source == "squidpy_seqfish" and 'celltype_mapped_refined' in adata.obs.columns:
                adata.obs['cluster'] = adata.obs['celltype_mapped_refined']

            adata.write(save_path)
            logger.info(f"âœ… Download & Formatting complete: {adata.shape}")
            return save_path
        except Exception as e:
            logger.error(f"âŒ Load failed: {e}")
            return None

    def preprocess_adata(self, adata_path: str):
        # ... (ä¿æŒåŸæœ‰çš„ preprocess_adata ä»£ç å®Œå…¨ä¸å˜) ...
        # è¯·åŠ¡å¿…ä¿ç•™ä¹‹å‰é‚£ç‰ˆåŒ…å« "target_colè¯†åˆ«é€»è¾‘" å’Œ "Markeræå–" çš„ä»£ç 
        logger.info(f"ğŸ”¬ Preprocessing {adata_path}...")
        adata = sc.read_h5ad(adata_path)
        
        adata.var_names = [g.upper() for g in adata.var_names]
        adata.var_names_make_unique()
        if adata.raw: del adata.raw
        
        # ç®€å•è´¨æ§
        sc.pp.filter_cells(adata, min_counts=self.cfg.min_counts)
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å½’ä¸€åŒ–
        max_val = adata.X.data.max() if sparse.issparse(adata.X) else adata.X.max()
        if max_val > 20:
            sc.pp.normalize_total(adata, target_sum=1e4)
            sc.pp.log1p(adata)

        # æ™ºèƒ½è¯†åˆ«èšç±»æ ‡ç­¾
        target_col = None
        # å¢åŠ  'annotation', 'celltype' ç­‰å¸¸è§åˆ—å
        for col in ['cluster', 'clusters', 'leiden', 'CellType', 'cell_type', 'annotation', 'celltype_mapped_refined']:
            if col in adata.obs.columns:
                target_col = col
                break
        
        if not target_col:
            logger.warning("âš ï¸ No annotation found, running Leiden...")
            sc.pp.pca(adata)
            sc.pp.neighbors(adata)
            sc.tl.leiden(adata)
            target_col = 'leiden'
        
        adata.obs['target_cluster'] = adata.obs[target_col].astype('category')
        logger.info(f"   Using annotation: '{target_col}'")
        
        # æ£€æŸ¥ç©ºé—´åæ ‡ (å…³é”®ï¼é’ˆå¯¹æ— å›¾åƒæ•°æ®)
        if 'spatial' not in adata.obsm:
            # å°è¯•å¯»æ‰¾å¸¸è§çš„åæ ‡åˆ—
            if 'X_spatial' in adata.obsm:
                adata.obsm['spatial'] = adata.obsm['X_spatial']
            elif 'spatial' in adata.uns:
                pass # Visiumæ ‡å‡†æ ¼å¼
            else:
                # æœ€åçš„å°è¯•ï¼šçœ‹obsé‡Œæœ‰æ²¡æœ‰ x, y æˆ– spatial_1, spatial_2
                candidates = [['x', 'y'], ['spatial_1', 'spatial_2'], ['x_centroid', 'y_centroid']]
                for c1, c2 in candidates:
                    if c1 in adata.obs.columns and c2 in adata.obs.columns:
                        adata.obsm['spatial'] = adata.obs[[c1, c2]].values
                        logger.info(f"   âœ… Constructed spatial coords from obs['{c1}', '{c2}']")
                        break
        
        # === [æ–°å¢] ç©ºé—´åæ ‡å½’ä¸€åŒ–ä¸è‡ªé€‚åº”é˜ˆå€¼è®¡ç®— ===
        logger.info("ğŸ“ Normalizing Spatial Coordinates & Auto-detecting Threshold...")
        
        # 1. è·å–åŸå§‹åæ ‡
        raw_coords = adata.obsm['spatial']
        
        # 2. å½’ä¸€åŒ–åˆ° [0, 1] åŒºé—´
        min_vals = raw_coords.min(axis=0)
        max_vals = raw_coords.max(axis=0)
        scale = max_vals - min_vals
        # é˜²æ­¢é™¤ä»¥0
        scale[scale == 0] = 1.0 
        
        norm_coords = (raw_coords - min_vals) / scale
        adata.obsm['spatial_norm'] = norm_coords # ä¿å­˜å½’ä¸€åŒ–åæ ‡ä¾›åç»­ä½¿ç”¨
        
        # 3. è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼ (åŸºäºæœ€è¿‘é‚»è·ç¦»)
        # é€»è¾‘ï¼šè®¡ç®—æ‰€æœ‰ç‚¹çš„å¹³å‡æœ€è¿‘é‚»è·ç¦» (Average Nearest Neighbor Distance)
        # è®¾å®šé˜ˆå€¼ä¸º NND çš„å€æ•° (ä¾‹å¦‚ 3-5 å€ï¼Œä»£è¡¨ 3-5 ä¸ªç»†èƒç›´å¾„/Spoté—´è·)
        from sklearn.neighbors import NearestNeighbors
        nbrs = NearestNeighbors(n_neighbors=2).fit(norm_coords)
        distances, _ = nbrs.kneighbors(norm_coords)
        
        # distances[:, 1] æ˜¯åˆ°æœ€è¿‘é‚»å±…çš„è·ç¦» (ç¬¬0ä¸ªæ˜¯è‡ªå·±)
        avg_nnd = np.mean(distances[:, 1])
        
        # åŠ¨æ€è®¾å®šé˜ˆå€¼ï¼šä¾‹å¦‚ 4 å€å¹³å‡é—´è· (å³å…è®¸è·¨è¶Š 3-4 ä¸ªç»†èƒ/Spot è¿›è¡Œé€šè®¯)
        adaptive_threshold = avg_nnd * 4.0
        
        # å°†å‚æ•°æ³¨å…¥åˆ° config ä¸­ä¾›åç»­ä½¿ç”¨
        self.cfg.adaptive_threshold = adaptive_threshold
        self.cfg.unit_scale = "normalized_units"
        
        logger.info(f"   âœ… Auto-Threshold: {adaptive_threshold:.4f} (based on 4x Avg NND: {avg_nnd:.4f})")
        logger.info(f"   âœ… Coordinates normalized to [0, 1].")
        
        # æå– Marker (ä¿æŒä¸å˜)
        logger.info(f"   Identifying Markers (Top {self.cfg.n_marker_genes})...")
        sc.tl.rank_genes_groups(adata, groupby='target_cluster', method='t-test', use_raw=False, n_genes=self.cfg.n_marker_genes)
        
        extracted_data = []
        for cid in adata.obs['target_cluster'].unique():
            try:
                df_markers = sc.get.rank_genes_groups_df(adata, group=cid)
                markers = df_markers['names'].tolist()
                clean_markers = [g for g in markers if not any(g.startswith(p) for p in self.cfg.blacklist_prefixes)]
                
                search_space = clean_markers[:200] 
                pairs = []
                for g1 in search_space:
                    for g2 in search_space:
                        if (g1, g2) in self.lr_db:
                            pairs.append(f"{g1} -> {g2}")
                
                if pairs:
                    pairs = list(set(pairs))[:50]
                    coords = adata.obsm['spatial'][adata.obs['target_cluster'] == cid]
                    extracted_data.append({
                        "cluster_id": str(cid),
                        "gene_list": clean_markers[:50],
                        "known_interactions": pairs,
                        "coordinates": coords.tolist()
                    })
            except Exception as e:
                pass

        logger.info(f"ğŸ“Š Extracted {len(extracted_data)} clusters.")
        return extracted_data, adata

# ==========================================
# 3. ç©ºé—´åˆ†æå™¨ (ä¿®æ­£æˆªæ–­é—®é¢˜)
# ==========================================
class SpaceAnalyzer:
    def __init__(self, config):
        self.cfg = config

    def _get_morphology_metrics(self, coords: np.ndarray, tissue_centroid: np.ndarray) -> str:
        """
        è®¡ç®—å½¢æ€å­¦ç‰¹å¾ (ä½¿ç”¨å½’ä¸€åŒ–åæ ‡è®¡ç®—ï¼Œä»¥ä¿è¯å°ºåº¦ç»Ÿä¸€)
        """
        if len(coords) < 4: return "Too few cells to determine morphology."
        
        try:
            # 1. è®¡ç®— MNND (Mean Nearest Neighbor Distance)
            # ä½¿ç”¨ k=2ï¼Œå› ä¸ºç¬¬1ä¸ªæœ€è¿‘é‚»æ˜¯è‡ªå·±
            nbrs = NearestNeighbors(n_neighbors=2).fit(coords)
            distances, _ = nbrs.kneighbors(coords)
            mnnd = np.mean(distances[:, 1]) 
            
            # 2. è®¡ç®—å‡¸åŒ…ä¸å¯†åº¦
            hull = scipy.spatial.ConvexHull(coords)
            volume = hull.volume
            # å¯†åº¦ = ç»†èƒæ•° / å½’ä¸€åŒ–ä½“ç§¯
            density = len(coords) / volume if volume > 0 else 0
            
            # 3. Global Localization (ç›¸å¯¹äºç»„ç»‡å‡ ä½•ä¸­å¿ƒçš„åç§»)
            cluster_centroid = np.mean(coords, axis=0)
            dist_to_center = np.linalg.norm(cluster_centroid - tissue_centroid)
            
            # 4. ç”Ÿæˆè¯­ä¹‰æè¿° (åŸºäºå½’ä¸€åŒ–åçš„ç»éªŒé˜ˆå€¼)
            # æ³¨æ„ï¼šè¿™é‡Œçš„é˜ˆå€¼å¯èƒ½éœ€è¦æ ¹æ®å½’ä¸€åŒ–åçš„åˆ†å¸ƒå¾®è°ƒï¼Œè¿™é‡Œä½¿ç”¨ç›¸å¯¹é€šç”¨çš„åˆ¤æ–­
            cohesion_desc = "highly cohesive" if density > 100 else "dispersed" 
            loc_desc = "peripheral" if dist_to_center > 0.4 else "central" # å½’ä¸€åŒ–åæ ‡èŒƒå›´ 0-1ï¼Œ0.4 ç®—è¾¹ç¼˜
            
            report = (
                f"   - Morphology: The cluster is {cohesion_desc} (Density: {density:.2f}, MNND: {mnnd:.4f}).\n"
                f"   - Localization: Located in the {loc_desc} region (Dist to center: {dist_to_center:.2f})."
            )
            return report
        except Exception as e:
            return f"Morphology calculation failed: {str(e)}"

    def verify_interaction(self, adata, ga, gb, cid):
        """
        éªŒè¯é…å—ä½“ç›¸äº’ä½œç”¨çš„ç©ºé—´è·ç¦»ã€‚
        
        [å…³é”®ä¿®è®¢]:
        1. ä¼˜å…ˆä½¿ç”¨ 'spatial_norm' (å½’ä¸€åŒ–åæ ‡)ã€‚
        2. Source é™åˆ¶åœ¨å½“å‰ Clusterï¼Œä½† Target æœç´¢å…¨ç»„ç»‡ (è§£å†³ Figure 5 éå¯¹è§’çº¿ä¸º0çš„é—®é¢˜)ã€‚
        """
        # 1. ç¡®å®š Source å€™é€‰æ± ï¼ˆå½“å‰ Cluster çš„ç´¢å¼•ï¼‰
        source_cluster_indices = np.where(adata.obs['target_cluster'].astype(str) == str(cid))[0]
        
        if len(source_cluster_indices) == 0: 
            return "Silent"

        try:
            # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨è·å–è¡¨è¾¾é‡
            def get_gene_expr(gene):
                try:
                    idx = adata.var_names.get_loc(gene)
                    col = adata.X[:, idx]
                    if sparse.issparse(col):
                        return col.toarray().flatten()
                    return col.flatten()
                except KeyError:
                    return None

            ea_full = get_gene_expr(ga) # é…ä½“å…¨é‡
            eb_full = get_gene_expr(gb) # å—ä½“å…¨é‡
            
            if ea_full is None or eb_full is None: 
                return "Error"
            
            # 2. æ„å»º Mask
            # Source Mask: å¿…é¡»å±äºå½“å‰ Cluster ä¸”è¡¨è¾¾é…ä½“
            mask_source = np.zeros(adata.n_obs, dtype=bool)
            mask_source[source_cluster_indices] = True
            mask_source = mask_source & (ea_full > 0)
            
            # Target Mask: ã€æ ¸å¿ƒä¿®å¤ã€‘å¯ä»¥æ˜¯ç»„ç»‡ä¸­ä»»ä½•è¡¨è¾¾å—ä½“çš„ç»†èƒ (Global Search)
            mask_target = eb_full > 0
            
            # 3. å¿«é€Ÿæ£€æŸ¥
            if mask_source.sum() == 0 or mask_target.sum() == 0:
                return "Silent"
            
            # æ£€æŸ¥å…±å®šä½ (åŒä¸€ä¸ªç»†èƒæ—¢è¡¨è¾¾Aåˆè¡¨è¾¾B)
            if (mask_source & mask_target).sum() > 5:
                return 0.0 
            
            # 4. å‡†å¤‡åæ ‡ (ä¼˜å…ˆä½¿ç”¨å½’ä¸€åŒ–åæ ‡)
            if 'spatial_norm' in adata.obsm:
                coords = adata.obsm['spatial_norm']
            else:
                # å›é€€æœºåˆ¶ï¼šå¦‚æœ DataManager æ²¡åšå½’ä¸€åŒ–ï¼Œå°±ç”¨åŸå§‹åæ ‡
                coords = adata.obsm['spatial']

            coords_source = coords[mask_source]
            coords_target = coords[mask_target]
            
            # 5. è®¡ç®—æœ€å°è·ç¦» (Paracrine Search)
            # cdist è®¡ç®— Source é›†åˆåˆ° Target é›†åˆçš„ä¸¤ä¸¤è·ç¦»çŸ©é˜µ
            dists = scipy.spatial.distance.cdist(coords_source, coords_target)
            
            # å–å…¨å±€æœ€å°å€¼ï¼šä»£è¡¨ä»è¯¥ Cluster å‘å‡ºçš„ä¿¡å·ï¼Œåˆ°è¾¾æœ€è¿‘å—ä½“çš„è·ç¦»
            min_dist = np.min(dists)
            
            return min_dist

        except Exception as e:
            return "Error"

    def generate_reports(self, json_data, adata, data_manager=None):
        """
        ç”ŸæˆåŒ…å«æ‹“æ‰‘ç‰¹å¾ã€ç©ºé—´éªŒè¯å’Œä¿¡å·å¼ºåº¦çš„ç»¼åˆæŠ¥å‘Š
        [ä¿®è®¢ç‰ˆ]: å¢åŠ äº†åŸºäº L*R è¡¨è¾¾é‡çš„ä¿¡å·å¼ºåº¦ (Signal Strength) è¯­ä¹‰æè¿°
        """
        logger.info("ğŸ§  Generating Spatial Reports with Knowledge & Intensity Injection...")
        
        # 1. å‡†å¤‡å…¨å±€åæ ‡å‚æ•°
        if 'spatial_norm' in adata.obsm:
            tissue_coords = adata.obsm['spatial_norm']
        else:
            tissue_coords = adata.obsm['spatial']
        tissue_centroid = np.mean(tissue_coords, axis=0)
        
        # 2. è·å–è‡ªé€‚åº”è·ç¦»é˜ˆå€¼
        tau = getattr(self.cfg, 'adaptive_threshold', 0.05)
        
        # 3. é¢„è®¡ç®—å…¨å±€å—ä½“è¡¨è¾¾å‡å€¼ (ç”¨äºè¯„ä¼°å—ä½“å¯ç”¨æ€§)
        #    ä¸ºäº†åŠ é€Ÿï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œåšä¸ªç®€å•çš„ç¼“å­˜æˆ–è€…ç›´æ¥åœ¨å¾ªç¯é‡Œå–
        #    è€ƒè™‘åˆ°å¾ªç¯æ¬¡æ•°ä¸å¤šï¼Œç›´æ¥å–ä¹Ÿè¡Œã€‚
        
        for entry in json_data:
            lines = []
            cid = entry['cluster_id']
            
            # --- Stream A: æ‹“æ‰‘å½¢æ€å­¦ ---
            mask = adata.obs['target_cluster'].astype(str) == str(cid)
            coords = tissue_coords[mask]
            morphology_report = self._get_morphology_metrics(coords, tissue_centroid)
            lines.append(f"1. Topological Context:\n{morphology_report}")
            
            # --- Stream C: ç©ºé—´å°ºåº¦ä¿¡æ¯ ---
            scale_info = (
                f"2. Spatial Scale Info:\n"
                f"   - Coordinate Space: Normalized [0, 1].\n"
                f"   - Interaction Threshold (Tau): {tau:.4f} units.\n"
                f"   - Logic: Interactions with dist < {tau:.4f} are verified. "
                f"Signal Strength is based on mass action (Ligand * Receptor)."
            )
            lines.append(scale_info)
            
            # --- Stream B: äº’ä½œéªŒè¯ä¸å¼ºåº¦è¯„ä¼° ---
            inter_lines = []
            valid_genes = set() 
            
            # é”å®šå½“å‰ Cluster çš„ç»†èƒç´¢å¼• (ç”¨äºè®¡ç®— Ligand å±€éƒ¨è¡¨è¾¾)
            cluster_cells = adata[mask]
            
            for pair in entry['known_interactions'][:40]: 
                ga, gb = pair.split(' -> ')
                
                # 1. è·ç¦»éªŒè¯
                res = self.verify_interaction(adata, ga, gb, cid)
                
                status = "Unknown"
                dist_val = float('inf')
                
                if isinstance(res, str): 
                    status = res
                else:
                    dist_val = res
                    if dist_val < 1e-5: status = "Co-localized (Autocrine)"
                    elif dist_val < tau: status = f"Physically Proximal (Dist: {dist_val:.4f} < Tau)"
                    else: status = f"Spatially Segregated (Dist: {dist_val:.4f} > Tau)"
                
                # 2. å¼ºåº¦è®¡ç®— (ä»…é’ˆå¯¹æœ‰æ•ˆäº’ä½œ)
                intensity_desc = ""
                if "Proximal" in status or "Co-localized" in status:
                    try:
                        # æ ¸å¿ƒé€»è¾‘ï¼šå¼ºåº¦ = å±€éƒ¨é…ä½“è¡¨è¾¾ * å…¨å±€å—ä½“è¡¨è¾¾
                        # (åæ˜ è¯¥ Cluster å‘å‡ºä¿¡å·çš„èƒ½åŠ› x ç»„ç»‡æ¥æ”¶ä¿¡å·çš„æ½œåŠ›)
                        
                        # è·å– Ligand åœ¨å½“å‰ Cluster çš„å‡å€¼
                        if sparse.issparse(cluster_cells.X):
                            idx_a = adata.var_names.get_loc(ga)
                            val_L = cluster_cells.X[:, idx_a].mean()
                        else:
                            val_L = cluster_cells[:, ga].X.mean()
                            
                        # è·å– Receptor åœ¨å…¨ç»„ç»‡çš„å‡å€¼ (æˆ–æ½œåœ¨ Target åŒºåŸŸï¼Œç®€åŒ–ä¸ºå…¨ç»„ç»‡)
                        if sparse.issparse(adata.X):
                            idx_b = adata.var_names.get_loc(gb)
                            val_R = adata.X[:, idx_b].mean()
                        else:
                            val_R = adata[:, gb].X.mean()
                        
                        # è®¡ç®—ä¹˜ç§¯ (Mass Action Proxy)
                        strength_score = val_L * val_R
                        
                        # è¯­ä¹‰åŒ–æ˜ å°„ (å‡è®¾æ•°æ®ç»è¿‡ log1p å¤„ç†ï¼Œå€¼é€šå¸¸åœ¨ 0-5 ä¹‹é—´)
                        # é˜ˆå€¼æ ¹æ®ç»éªŒè®¾å®šï¼Œå¯å¾®è°ƒ
                        if strength_score > 1.0: i_tag = "**Very High**"
                        elif strength_score > 0.5: i_tag = "High"
                        elif strength_score > 0.1: i_tag = "Moderate"
                        else: i_tag = "Low"
                        
                        intensity_desc = f" | Signal Strength: {i_tag}"
                        
                        valid_genes.add(ga)
                        valid_genes.add(gb)
                        
                    except Exception as e:
                        # å®¹é”™ï¼šå¦‚æœè®¡ç®—å¤±è´¥ï¼Œä¸æŠ¥é”™ï¼Œåªç•™ç©º
                        pass
                    
                inter_lines.append(f"- {ga} -> {gb}: {status}{intensity_desc}")
            
            lines.append("3. Interaction Verification Log:\n" + ("\n".join(inter_lines) if inter_lines else "None"))
            
            # --- Stream D: å¤–éƒ¨æ•°æ®åº“çŸ¥è¯†æ³¨å…¥ ---
            if data_manager and hasattr(data_manager, 'func_db'):
                kb_lines = []
                for g in list(valid_genes)[:15]:
                    if g in data_manager.func_db:
                        funcs = "; ".join(data_manager.func_db[g][:2]) 
                        kb_lines.append(f"   - {g}: {funcs}")
                
                if kb_lines:
                    lines.append(f"4. Functional Annotations (Background Knowledge):\n" + "\n".join(kb_lines))
                else:
                    lines.append("4. Functional Annotations: None available for valid targets.")
            
            # åˆå¹¶ Prompt
            entry['spatial_context'] = f"Cluster {cid} Analysis Report:\n" + "\n\n".join(lines)
            
        return json_data

# ==========================================
# 4. Agent æ¥å£
# ==========================================
# ==========================================
# 4. Agent æ¥å£ (Revised with GPT-4 Support)
# ==========================================
class AgentInterface:
    def __init__(self, config: SpaceConfig):
        self.cfg = config
        self.pipe = None
        self.client = None

    def load_model(self):
        """Initializes either the Local LLM or the OpenAI API Client"""
        if self.cfg.llm_source == "api":
            logger.info(f"ğŸš€ Initializing OpenAI API Client ({self.cfg.openai_model})...")
            try:
                from openai import OpenAI
                self.client = OpenAI(
                    api_key=self.cfg.openai_api_key,
                    base_url=self.cfg.openai_base_url
                )
                logger.info("âœ… OpenAI Client connected.")
            except ImportError:
                logger.error("âŒ 'openai' library not installed. Please run: pip install openai")
            except Exception as e:
                logger.error(f"âŒ OpenAI init failed: {e}")
        
        else:
            # Local Mode (GPT, etc.)
            logger.info(f"ğŸš€ Loading Local LLM: {self.cfg.model_id}...")
            try:
                self.pipe = pipeline(
                    "text-generation", 
                    model=self.cfg.model_id, 
                    device=self.cfg.gpu_id, 
                    torch_dtype=torch.bfloat16, 
                    trust_remote_code=True
                )
            except Exception as e:
                logger.error(f"âŒ Local model load failed: {e}")

    def _call_gpt4(self, messages: List[Dict]) -> str:
        """Helper to call GPT-4 API"""
        if not self.client: return ""
        try:
            response = self.client.chat.completions.create(
                model=self.cfg.openai_model,
                messages=messages,
                temperature=0.2, # Low temp for scientific rigor
                max_tokens=self.cfg.max_new_tokens
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.warning(f"âš ï¸ GPT-4 API Error: {e}")
            return ""

    def run_inference(self, json_data):
        if (self.cfg.llm_source == "local" and not self.pipe) and \
           (self.cfg.llm_source == "api" and not self.client):
            logger.error("âŒ No valid model/client loaded.")
            return json_data

        logger.info(f"ğŸ¤– SpaceAgent Reasoning (Source: {self.cfg.llm_source})...")
        results = []
        
        for i, entry in enumerate(json_data):
            # Construct Prompt
            prompt_messages = [
                {"role": "system", "content": "You are a spatial biologist. Reject interactions marked 'Segregated' or 'Silent'. Accept 'Proximal' or 'Co-localized'. Focus on biological plausibility based on the gene functions provided."},
                {"role": "user", "content": f"Genes: {entry.get('gene_list', [])[:20]}\nContext: {entry.get('spatial_context', '')}\n\nAnalyze valid interactions:"}
            ]
            
            output_text = ""
            
            # Branch Logic: API vs Local
            if self.cfg.llm_source == "api":
                output_text = self._call_gpt4(prompt_messages)
            else:
                # Local Pipeline
                try:
                    out = self.pipe(prompt_messages, max_new_tokens=self.cfg.max_new_tokens, do_sample=False)
                    output_text = out[0]["generated_text"][-1]["content"]
                except Exception as e:
                    logger.warning(f"âš ï¸ Local Inference Error: {e}")

            if output_text:
                entry['llm_analysis'] = output_text
                results.append(entry)
                
            # Optional: Print progress for long API runs
            if (i + 1) % 5 == 0:
                logger.info(f"   Processed {i + 1}/{len(json_data)} clusters...")

        return results